{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Making People Smile with VAEs\n",
        "\n",
        "In this notebook we implement an interesting computational experiment. First, we train a convolutional Variational Autoencoder on the CelebA dataset and then we attemp to add a smile on the generated faces by performing some arithmetic in the latent space. (Inspired by William Rabschka)\n",
        "\n",
        "Note: Lately there seems to be an issue with loading the dataset directly from torchvision (*to fix).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6FCmlMkScPEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Imports"
      ],
      "metadata": {
        "id": "9bUof8jnecP9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX0XU9aUPBio",
        "outputId": "ba9380ab-c8e8-4d61-deec-b9e713a8ed25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "import time\n",
        "\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data\n",
        "\n",
        "# Training Set\n",
        "train_dataset = datasets.CelebA(root='data',\n",
        "                                    split='train',\n",
        "                                    transform=transforms.ToTensor(),\n",
        "                                    download=True)\n",
        "\n",
        "train_dataset = datasets.CelebA(root='data',\n",
        "                                    split='test',\n",
        "                                    transform=transforms.ToTensor(),\n",
        "                                    download=True)\n"
      ],
      "metadata": {
        "id": "rMUG8GNpsuTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up hyperparameters and device\n",
        "\n",
        "# Device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Hyperparameters\n",
        "random_seed = 42\n",
        "lr = 0.005\n",
        "epochs = 50\n",
        "batch_size = 256\n",
        "\n",
        "# Check\n",
        "print(f'We are using {device} to train our model.')"
      ],
      "metadata": {
        "id": "jsDj77Y6s2OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Reshape(nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "        self.shape = args\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.view(self.shape)\n",
        "\n",
        "\n",
        "class Trim(nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :128, :128]\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "                nn.Conv2d(3, 32, stride=2, kernel_size=3, bias=False, padding=1),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.LeakyReLU(0.1, inplace=True),\n",
        "                nn.Dropout2d(0.25),\n",
        "                #\n",
        "                nn.Conv2d(32, 64, stride=2, kernel_size=3, bias=False, padding=1),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.LeakyReLU(0.1, inplace=True),\n",
        "                nn.Dropout2d(0.25),\n",
        "                #\n",
        "                nn.Conv2d(64, 64, stride=2, kernel_size=3, bias=False, padding=1),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.LeakyReLU(0.1, inplace=True),\n",
        "                nn.Dropout2d(0.25),\n",
        "                #\n",
        "                nn.Conv2d(64, 64, stride=2, kernel_size=3, bias=False, padding=1),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.LeakyReLU(0.1, inplace=True),\n",
        "                nn.Dropout2d(0.25),\n",
        "                #\n",
        "                nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        self.z_mean = torch.nn.Linear(4096, 200)\n",
        "        self.z_log_var = torch.nn.Linear(4096, 200)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "                torch.nn.Linear(200, 4096),\n",
        "                Reshape(-1, 64, 8, 8),\n",
        "                #\n",
        "                nn.ConvTranspose2d(64, 64, stride=2, kernel_size=3),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.LeakyReLU(0.1, inplace=True),\n",
        "                nn.Dropout2d(0.25),\n",
        "                #\n",
        "                nn.ConvTranspose2d(64, 64, stride=2, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.LeakyReLU(0.1, inplace=True),\n",
        "                nn.Dropout2d(0.25),\n",
        "                #\n",
        "                nn.ConvTranspose2d(64, 32, stride=2, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.LeakyReLU(0.1, inplace=True),\n",
        "                nn.Dropout2d(0.25),\n",
        "                #\n",
        "                nn.ConvTranspose2d(32, 3, stride=2, kernel_size=3, padding=1),\n",
        "                #\n",
        "                Trim(),  # 3x129x129 -> 3x128x128\n",
        "                nn.Sigmoid()\n",
        "                )\n",
        "\n",
        "    def reparameterize(self, z_mu, z_log_var):\n",
        "        eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(z_mu.get_device())\n",
        "        z = z_mu + eps * torch.exp(z_log_var/2.)\n",
        "        return z\n",
        "\n",
        "\n",
        "    def encoding_fn(self, x):\n",
        "        x = self.encoder(x)\n",
        "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
        "        encoded = self.reparameterize(z_mean, z_log_var)\n",
        "        return encoded\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
        "        encoded = self.reparameterize(z_mean, z_log_var)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, z_mean, z_log_var, decoded"
      ],
      "metadata": {
        "id": "N6Qa19pzfUNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training Set:\\n')\n",
        "for images,labels in train_loader:\n",
        "  print('Image batch dimensions:', images.size())\n",
        "  print('Image label dimensions:', labels.size())\n",
        "  break\n",
        "\n",
        "\n",
        "print('\\nTest Set:\\n')\n",
        "for images,labels in test_loader:\n",
        "  print('Image batch dimensions:', images.size())\n",
        "  print('Image label dimensions:', labels.size())\n",
        "  break\n"
      ],
      "metadata": {
        "id": "-VAqOe8ts8ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "\n",
        "torch.manual_seed(random_seed)\n",
        "model = VAE()\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = lr)"
      ],
      "metadata": {
        "id": "V9KamOZstGTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model summary\n",
        "!pip install torchinfo\n",
        "\n",
        "from torchinfo import summary\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "SkiJxEbMtJyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def compute_epoch_loss_autoencoder(model, data_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    curr_loss, num_examples = 0., 0\n",
        "    with torch.no_grad():\n",
        "        for features, _ in data_loader:\n",
        "            features = features.to(device)\n",
        "            logits = model(features)\n",
        "            loss = loss_fn(logits, features, reduction='sum')\n",
        "            num_examples += features.size(0)\n",
        "            curr_loss += loss\n",
        "\n",
        "        curr_loss = curr_loss / num_examples\n",
        "        return curr_loss\n",
        "\n",
        "\n",
        "def train_vae_v1(num_epochs, model, optimizer, device,\n",
        "                 train_loader, loss_fn=None,\n",
        "                 logging_interval=100,\n",
        "                 skip_epoch_stats=False,\n",
        "                 reconstruction_term_weight=1,\n",
        "                 save_model=None):\n",
        "\n",
        "    log_dict = {'train_combined_loss_per_batch': [],\n",
        "                'train_combined_loss_per_epoch': [],\n",
        "                'train_reconstruction_loss_per_batch': [],\n",
        "                'train_kl_loss_per_batch': []}\n",
        "\n",
        "    if loss_fn is None:\n",
        "        loss_fn = F.mse_loss\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx, (features, _) in enumerate(train_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "\n",
        "            # FORWARD AND BACK PROP\n",
        "            encoded, z_mean, z_log_var, decoded = model(features)\n",
        "\n",
        "            # total loss = reconstruction loss + KL divergence\n",
        "            #kl_divergence = (0.5 * (z_mean**2 +\n",
        "            #                        torch.exp(z_log_var) - z_log_var - 1)).sum()\n",
        "            kl_div = -0.5 * torch.sum(1 + z_log_var\n",
        "                                      - z_mean**2\n",
        "                                      - torch.exp(z_log_var),\n",
        "                                      axis=1) # sum over latent dimension\n",
        "\n",
        "            batchsize = kl_div.size(0)\n",
        "            kl_div = kl_div.mean() # average over batch dimension\n",
        "\n",
        "            pixelwise = loss_fn(decoded, features, reduction='none')\n",
        "            pixelwise = pixelwise.view(batchsize, -1).sum(axis=1) # sum over pixels\n",
        "            pixelwise = pixelwise.mean() # average over batch dimension\n",
        "\n",
        "            loss = reconstruction_term_weight*pixelwise + kl_div\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # UPDATE MODEL PARAMETERS\n",
        "            optimizer.step()\n",
        "\n",
        "            # LOGGING\n",
        "            log_dict['train_combined_loss_per_batch'].append(loss.item())\n",
        "            log_dict['train_reconstruction_loss_per_batch'].append(pixelwise.item())\n",
        "            log_dict['train_kl_loss_per_batch'].append(kl_div.item())\n",
        "\n",
        "            if not batch_idx % logging_interval:\n",
        "                print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f'\n",
        "                      % (epoch+1, num_epochs, batch_idx,\n",
        "                          len(train_loader), loss))\n",
        "\n",
        "        if not skip_epoch_stats:\n",
        "            model.eval()\n",
        "\n",
        "            with torch.set_grad_enabled(False):  # save memory during inference\n",
        "\n",
        "                train_loss = compute_epoch_loss_autoencoder(\n",
        "                    model, train_loader, loss_fn, device)\n",
        "                print('***Epoch: %03d/%03d | Loss: %.3f' % (\n",
        "                      epoch+1, num_epochs, train_loss))\n",
        "                log_dict['train_combined_per_epoch'].append(train_loss.item())\n",
        "\n",
        "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
        "\n",
        "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
        "    if save_model is not None:\n",
        "        torch.save(model.state_dict(), save_model)\n",
        "\n",
        "    return log_dict"
      ],
      "metadata": {
        "id": "1NWgCrFmtQZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dict = train_vae_v1(num_epochs=epochs, model=model,\n",
        "                        optimizer=optimizer, device=device,\n",
        "                        train_loader=train_loader,\n",
        "                        skip_epoch_stats=True,\n",
        "                        logging_interval=50)"
      ],
      "metadata": {
        "id": "O35MSPHvtReC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def plot_training_loss(minibatch_losses, num_epochs, averaging_iterations=100, custom_label=''):\n",
        "\n",
        "    iter_per_epoch = len(minibatch_losses) // num_epochs\n",
        "\n",
        "    plt.figure()\n",
        "    ax1 = plt.subplot(1, 1, 1)\n",
        "    ax1.plot(range(len(minibatch_losses)),\n",
        "             (minibatch_losses), label=f'Minibatch Loss{custom_label}')\n",
        "    ax1.set_xlabel('Iterations')\n",
        "    ax1.set_ylabel('Loss')\n",
        "\n",
        "    if len(minibatch_losses) < 1000:\n",
        "        num_losses = len(minibatch_losses) // 2\n",
        "    else:\n",
        "        num_losses = 1000\n",
        "\n",
        "    ax1.set_ylim([\n",
        "        0, np.max(minibatch_losses[num_losses:])*1.5\n",
        "        ])\n",
        "\n",
        "    ax1.plot(np.convolve(minibatch_losses,\n",
        "                         np.ones(averaging_iterations,)/averaging_iterations,\n",
        "                         mode='valid'),\n",
        "             label=f'Running Average{custom_label}')\n",
        "    ax1.legend()\n",
        "\n",
        "    ###################\n",
        "    # Set scond x-axis\n",
        "    ax2 = ax1.twiny()\n",
        "    newlabel = list(range(num_epochs+1))\n",
        "\n",
        "    newpos = [e*iter_per_epoch for e in newlabel]\n",
        "\n",
        "    ax2.set_xticks(newpos[::10])\n",
        "    ax2.set_xticklabels(newlabel[::10])\n",
        "\n",
        "    ax2.xaxis.set_ticks_position('bottom')\n",
        "    ax2.xaxis.set_label_position('bottom')\n",
        "    ax2.spines['bottom'].set_position(('outward', 45))\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_xlim(ax1.get_xlim())\n",
        "    ###################\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "U4XS7KRntan2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_loss(log_dict['train_reconstruction_loss_per_batch'], epochs, custom_label=\" (reconstruction)\")\n",
        "plot_training_loss(log_dict['train_kl_loss_per_batch'], epochs, custom_label=\" (KL)\")\n",
        "plot_training_loss(log_dict['train_combined_loss_per_batch'], epochs, custom_label=\" (combined)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NKyUWX-HtbqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute Average Faces in the Latent Space"
      ],
      "metadata": {
        "id": "_X2waZvFtto9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_average_faces(feature_idx, image_dim, data_loader, device=None, encoding_fn=None):\n",
        "\n",
        "    avg_img_with_feat = torch.zeros(image_dim, dtype=torch.float32)\n",
        "    avg_img_without_feat = torch.zeros(image_dim, dtype=torch.float32)\n",
        "\n",
        "    num_img_with_feat = 0\n",
        "    num_images_without_feat = 0\n",
        "\n",
        "    for images, labels in data_loader:\n",
        "        idx_img_with_feat = labels[:, feature_idx].to(torch.bool)\n",
        "\n",
        "        if encoding_fn is None:\n",
        "            embeddings = images\n",
        "        else:\n",
        "            ####################################\n",
        "            ### Get latent representation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                if device is not None:\n",
        "                    images = images.to(device)\n",
        "                embeddings = encoding_fn(images).to('cpu')\n",
        "            ####################################\n",
        "\n",
        "        avg_img_with_feat += torch.sum(embeddings[idx_img_with_feat], axis=0)\n",
        "        avg_img_without_feat += torch.sum(embeddings[~idx_img_with_feat], axis=0)\n",
        "        num_img_with_feat += idx_img_with_feat.sum(axis=0)\n",
        "        num_images_without_feat += (~idx_img_with_feat).sum(axis=0)\n",
        "\n",
        "    avg_img_with_feat /= num_img_with_feat\n",
        "    avg_img_without_feat /= num_images_without_feat\n",
        "\n",
        "    return avg_img_with_feat, avg_img_without_feat"
      ],
      "metadata": {
        "id": "Irb9_bL2uHLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_img_with_feat, avg_img_without_feat = compute_average_faces(\n",
        "    feature_idx=31, # smiling\n",
        "    image_dim=200,\n",
        "    data_loader=train_loader,\n",
        "    device=DEVICE,\n",
        "    encoding_fn=model.encoding_fn)"
      ],
      "metadata": {
        "id": "H1p4lHHbtegc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More or Less Smiling"
      ],
      "metadata": {
        "id": "5pUlmp96ub1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diff = (avg_img_with_feat - avg_img_without_feat)\n",
        "\n",
        "example_img = EXAMPLE_IMAGE.unsqueeze(0).to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    encoded = model.encoding_fn(example_img).squeeze(0).to('cpu')\n",
        "\n",
        "plot_modified_faces(original=encoded,\n",
        "                    decoding_fn=model.decoder,\n",
        "                    device=DEVICE,\n",
        "                    diff=diff)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DrgXG0kaugjE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}